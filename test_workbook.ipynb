{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with Memory from txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"test_data.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1049, which is longer than the specified 1000\n",
      "Created a chunk of size 1049, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"From whom is the Document?\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The document is from Linea Schmidt.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Is there any information about her profession?\"\n",
    "result2 = qa({\"question\": query2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with Memory from Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do agents use Task decomposition?',\n",
       " 'chat_history': [SystemMessage(content='')],\n",
       " 'answer': 'Agents utilize task decomposition by breaking down large tasks into smaller, manageable subgoals. This allows them to efficiently handle complex tasks. Task decomposition can be achieved through different methods such as using simple prompting, task-specific instructions, or human inputs. The agent can then plan ahead and determine the necessary steps to achieve each subgoal. This approach enables the agent to tackle complicated tasks in a more organized and systematic manner.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"How do agents use Task decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the various ways to implement memory to support it?',\n",
       " 'chat_history': [SystemMessage(content='\\nThe human asks how agents use Task decomposition. The AI explains that agents utilize task decomposition by breaking down large tasks into smaller, manageable subgoals, which can be achieved through different methods such as using simple prompting, task-specific instructions, or human inputs. This allows the agent to plan ahead and determine the necessary steps to achieve each subgoal, enabling them to tackle complicated tasks in a more organized and systematic manner.')],\n",
       " 'answer': \"The different methods to implement memory in order to support task decomposition are:\\n\\n1. Long Term Memory Management: This involves storing and organizing information in a long-term memory system. It allows the agent to retain knowledge of past tasks, subgoals, and their corresponding solutions. This memory can be accessed during task decomposition to retrieve relevant information and strategies.\\n\\n2. Internet Access: The agent can have access to the internet for searches and information gathering. This allows the agent to retrieve external information that may be useful for task decomposition and subgoal identification.\\n\\n3. GPT-3.5 Powered Agents: These agents can be used for delegation of simple tasks. They can store and retrieve information from their memory to assist in task decomposition. GPT-3.5 Powered Agents can provide suggestions and guidance based on their training data and previous experiences.\\n\\n4. File Output: The agent can store information and progress in files or documents. This allows for easy reference and retrieval of previous task decomposition steps. File output can be used to track the agent's progress, reflect on past actions, and refine the task decomposition process.\\n\\nThese memory methods support task decomposition by providing the agent with access to relevant information, previous experiences, and external resources, enabling efficient and effective handling of complex tasks.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What are the various ways to implement memory to support it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query from Neo4J database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jLoader:\n",
    "    def __init__(self, connection, query):\n",
    "        self.connection = connection\n",
    "        self.query = query\n",
    "\n",
    "    def load(self):\n",
    "        with self.connection._driver.session() as session:\n",
    "            results = session.run(self.query)\n",
    "            data = [record for record in results]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jService(object):\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        with self._driver.session() as session:\n",
    "            return session.read_transaction(self._fetch_data_from_db)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fetch_data_from_db(tx):\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Study)  // Assuming you have a label 'Study' for your nodes\n",
    "        RETURN n.description  // Assuming each study node has a 'description' property\n",
    "        \"\"\"\n",
    "        result = tx.run(query)\n",
    "        return [record[\"n.description\"] for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/gd9db9l10k5grhsvfgxwyll00000gn/T/ipykernel_38522/4294405743.py:11: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  return session.read_transaction(self._fetch_data_from_db)\n"
     ]
    }
   ],
   "source": [
    "# Use your Neo4j credentials here\n",
    "neo_service = Neo4jService(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "data = neo_service.fetch_data()\n",
    "neo_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_splitter \u001b[39m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m all_splits \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39;49msplit_documents(data)\n\u001b[1;32m      4\u001b[0m vectorstore \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(documents\u001b[39m=\u001b[39mall_splits, embedding\u001b[39m=\u001b[39mOpenAIEmbeddings())\n\u001b[1;32m      5\u001b[0m memory \u001b[39m=\u001b[39m ConversationSummaryMemory(llm\u001b[39m=\u001b[39mllm, memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m, return_messages\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/genai-stack-u9ltUab-/lib/python3.9/site-packages/langchain/text_splitter.py:159\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    157\u001b[0m texts, metadatas \u001b[39m=\u001b[39m [], []\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents:\n\u001b[0;32m--> 159\u001b[0m     texts\u001b[39m.\u001b[39mappend(doc\u001b[39m.\u001b[39;49mpage_content)\n\u001b[1;32m    160\u001b[0m     metadatas\u001b[39m.\u001b[39mappend(doc\u001b[39m.\u001b[39mmetadata)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_documents(texts, metadatas\u001b[39m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
    "\n",
    "response = qa(\"How do agents use Task decomposition?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-stack-u9ltUab-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
